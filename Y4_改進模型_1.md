# 🤡 Improvements/改進模型 (上)

> 筆者：109612011 葉騏緯

## 加入注意力機制

你可以對 YOLOv5 提供的 code 進行一些魔改，像是打開 `common.py` ，加入下面的 code ：

```python
class SE(nn.Module):
    def __init__(self, c1, c2, ratio=16):
        super(SE, self).__init__()
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.l1 = nn.Linear(c1, c1 // ratio, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.l2 = nn.Linear(c1 // ratio, c1, bias=False)
        self.sig = nn.Sigmoid()
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avgpool(x).view(b, c)
        y = self.l1(y)
        y = self.relu(y)
        y = self.l2(y)
        y = self.sig(y)
        y = y.view(b, c, 1, 1)
        return x * y.expand_as(x)
```

然後修改模型YAML檔中的架構 ([準備工作](/Y2_準備工作.md) 中有提及哪裡是架構)。
<img src="https://imgur.com/8B0aHEN.png"  style="display:block;margin:auto;width:90%;height:auto;">

 >⚠ 注意  
 >`backbone` 替換完記得 `head` 也要改喔。

改完後，你訓練模型時應該會看到有成功加上去。
<img src="https://imgur.com/Eg4SQdW.png"  style="display:block;margin:auto;width:90%;height:auto;">
 <center> 嘿，對，這邊可以看架構。YAML檔看不懂的話，看一下這邊應該就懂在寫啥了吧</center><br>

看到這，你的下一句要說的話是：「這到底在幹嘛......」

<img src="https://i.imgur.com/AXgifun.jpg"  style="display:block;margin:auto;width:90%;height:auto;"><br>

不用緊張，這部分不先介紹一下基本知識是不行的。接下來的部分會講筆者自己對 Attention 的了解

## 什麼是 Self-attention？

Self-attention 的存在，是為了要處理「當你的 input 是 sequence」的情境。Sequence 可能是各種資料型態，它可以是一段句子，以一個 token 為單位；它可以是一段語音訊號，以一個 frame 為單位；它也可以是你輸入的一張圖片，以一個 pixel 為單位。

在學習模型時，我們常常聽到一些很形象化的敘述，這當然能很好的幫助我們理解模型**大致**在幹什麼。不過，當你真的要開始實作就會發現，你仍然一無所知。所以我們還需要再了解深入一點。

### 理解前的準備

這邊有個例子，告訴你我為什麼要講比較深入點：

<details>
<summary><b>例子</b> (不嫌囉嗦你可以點開來看)</summary>

假設今天你在學習演算法，我們以經典的 Binary Search (二分搜尋法) 為例好了：

> 二分搜尋演算法的精神就是你翻開一本書的某頁，想找某段文句是否在這頁，如果它在這頁，恭喜；但如果它不在這頁，你現在就可以確定目標要不在這頁前，要不在這頁後。  

現在，你或許會覺得這樣的比喻相當傳神，你期考 Open Book 也是這樣在找答案，你好像聽懂了。但如果我現在叫你立刻用 `C++` 寫一個 function 來實作，你確定你有辦法立刻做對？應該是不行，因為這樣的比喻缺失了很多細節，比如：你必須確定這是筆有序的資料，也就是說可能要先做排序。

但其實，先有了形象化的大框架，再來補充細節，你實作的時候就會更了解自己在幹嘛。或者，先看答案，再來三明治解法，細看框架。

</details><br>

要更好的理解 Self-attention，先建立一個形象：

> 整個 Self-attention module，我們可以想成是由好多傳送門連結在一起構成的，而這些傳送門就是一些**數學運算**。Module 本身則是組成一個大網路 (a large network) 的**其中一塊**積木。

如果你的腦海中已經有了深刻的認知了，那麼我們就可以開始講下去了。

### 數學傳送門

Sequence 我們可以看成是 vector set，說到 vector 大家就不陌生了。

Self-attetion 會吃進一整個 sequence，你 input 多少個 vector，它就會 output 幾個。

<img src="https://imgur.com/qSs9oCy.png"  style="display:block;margin:auto;width:90%;height:auto;"><br>

$a$ 是我們 input 的向量， $b$ 是我們得到的 output。對各個 $b$ 來說，它都是考慮了每個 $a$ 後才得到的。

<img src="https://imgur.com/14QIEg7.png"  style="display:block;margin:auto;width:90%;height:auto;"><br>

那麼，怎麼**考慮**呢？

比較常見的一種作法就是內積 (dot product)。

以計算 $b_1$ 為例，首先我們必須計算每個 $a$ 跟 $a_1$ 的關聯程度 (包括它自己)。

我們以 $\alpha$ 表示這個關聯度，其中 $\alpha$ 是 q (query) 跟 k (key) 的內積，也就是 $\alpha = q\cdot k$。  
$q$ 跟 $k$ 我們可以透過將 $a$ 乘上對應的矩陣來計算出：

$$
\begin{cases}
q_i &= W_qa_i\\
k_i &= W_ka_i
\end{cases}
$$

如此一來，我們可以求出一個 $q_1$ 以及 $k_1$, $k_2$, $k_3$, $k_4$，並計算出每個 $a$ 跟 $a_1$ 的關聯度： $\alpha_{1,1}$, $\alpha_{1,2}$, $\alpha_{1,3}$, $\alpha_{1,4}$。

接著，我們給每個 $\alpha$ 都過一層 [Soft-Max](https://zh.wikipedia.org/zh-tw/Softmax%E5%87%BD%E6%95%B0)，得到  $\alpha'_{1,1}$, $\alpha'_{1,2}$, $\alpha'_{1,3}$, $\alpha'_{1,4}$。這邊會得到一個機率分布那樣的結果，你不一定要用 Soft-Max，你也嘗試用 ReLU 之類的各種 Activation function。

之後，我們再來求 $v$ (value)，它會是將 $a$ 乘上一個矩陣，將 $\alpha$ 都乘上 $v$ 後全部相加，即可求得 $b_1$：

$$
\begin{cases}
v_i &= W_va_i\\
b_1 &= \sum\limits _i v_i \alpha'_{1,i}
\end{cases}
$$

這個想法其實就是在做**加權**，讓關聯性大者對 $b$ 的值造成較多影響。

我們用簡單的矩陣乘法把上述總結：

如果我們 sequence 的長度為 $N$,那麼就會有長度亦為 $N$ 的 query 跟 key。將兩者內積我們可以得到大小為 $N\times N$ 的矩陣 $A$ 裡面裝著關聯度 $\alpha$。將 $A$ 通過 Soft-max 得到 $A'$，再讓 value 乘上 $A'$，最後就能得到我們想要的：

$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

註： $\sqrt{d_k}$ 是用來調整內積大小用的。

一頓操作猛如虎下來你會發現，self-attention layer 裡面唯一需要學的參數就是那三個矩陣：$W_q$、 $W_k$ 及 $W_v$ 而已。

還有另一類叫做 multi-head self-attention，作法就是讓每個 $b$ 由許多個 $q$、 $k$、 $v$ 先求得向量 $b_{i,1}, b_{i,2}...$ 讓這向量過一個矩陣後才算出 $b_i$。算一種變形。

#### Positional Encoding

看到這邊你也許會發現，上述少考慮了一項或許很重要的資訊，也就是**位置**。我們是一次吃進去整個 sequence，然後同時求出每個 $b$。

這邊的解決辦法很簡單，我們將計算出的 positional vector $e_i$ 來代表位置的資訊，再加到 $a_i$ 上即可。至於怎麼計算出呢，這邊就可以有你的巧思了。在發揚光大 self-attention 的論文 [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) 中，他們的很巧妙的使用了 $\sin$ 跟 $\cos$ 來表示。

Attention Is All You Need 呢就是提出大名鼎鼎的 Transformer 的那篇論文，也是上面那個公式引用的來源。我自己是從頭到尾讀完後，對 self-attetion 有了相當深刻的理解。

### 影像處理如何應用 Self-Attention 🤨？

其實就一個想法：我們可以**把圖片看成 vector set**。

以一個 $M\times N$ 的圖片來說，我們把它看成一個 $M\times N\times 3$ 的 tensor (3 代表 RGB 3 個 channel)。這樣不就能交給 self-attention 了嗎？

甚至，我們其實可以把 CNN 看成一種簡化版的 self-attention (CNN 我們只考慮 receptive field 裡面的 data，而 self-attention 則考慮了整張圖片)。

### Self-attention 的問題

Self-attention 最大的問題就是運算量很大，如何減少計算量也就成了一個研究的主題。Self-attention 的運算量跟 $N$ 有關，當你的 $N$ 特別大時，self-attention 會影響運算量最多；換句話說，如果你的 self-attention 並未 dominates computation，那你 self-attention 就算換改良後的變體，幫助可能也不大。

- - -

[下一篇](/Y4_改進模型_2.md)，我們將繼續介紹 self-attention 的一些變化型。
